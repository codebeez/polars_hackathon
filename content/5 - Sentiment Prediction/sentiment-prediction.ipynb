{"cells":[{"cell_type":"markdown","id":"687c8139","metadata":{},"source":["# Sentiment Prediction"]},{"cell_type":"markdown","id":"33359128","metadata":{},"source":["In this last part of the hackathon, we will train a small deep learning model on the IMDB Sentiment dataset.\n","This dataset contains review texts along with a labelled \"positive\" or \"negative\" sentiment.\n","We will preprocess the data using Polars, which you will implement.\n","We will then train and evaluate a neural network that can predict a sentiment for an arbitrary input text!"]},{"cell_type":"code","execution_count":9,"id":"a001b8cc","metadata":{"id":"4e50183c"},"outputs":[],"source":["import joblib\n","import polars as pl\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, LSTM\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"markdown","id":"9315d7f8","metadata":{},"source":["## Load the data\n","First of all we will load the data.\n","It is always a good idea to visualise a portion of the data and do a few sanity checks before starting the preprocessing.\n","\n","### Exercise 5.1\n","#### Exercise 5.1.1\n","Load the data using polars.\n","Since in this case we are going to process all the data at once, we can read it in directly rather than scanning and collecting separate parts in our analysis.\n","However, a golden tip is that it is still always a good idea convert the DataFrame to a LazyFrame to allow the optimization engine to optimize all the queries we define subsequently."]},{"cell_type":"code","execution_count":10,"id":"0ac09bfa","metadata":{"id":"f3d9a22a"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"3cbc42d6","metadata":{},"source":["#### Exercise 5.1.2\n","Using only Polars, show the number of records in the dataset, (separately) visualise the first few records and the last few records of the data and show the counts per sentiment value."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6b3669a9","outputId":"a20d5236-e53a-4fb7-b770-605eda4f68e4"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"bb9c24e1","metadata":{"id":"a3de2414"},"source":["## Preprocessing\n","In case of a larger dataset, it may be a good idea to use the `tf.Dataset` interface to allow preprocessing portions of the data in parallel with training on the data that has already been preprocessed, which can be accomplished through the `map()` function.\n","In this case, our dataset is relatively small and we can do the full preprocessing beforehand.\n","\n","First off, for the training procedure we will want to use integer values for our labels.\n","In the case of binary values, mapping to 0 and 1 is most commonly used.\n","\n","### Exercise 5.2\n","Convert the sentiment column in the data frame by mapping positive reviews to the value 1 and negative reviews to the value 0.\n","Display the first few records to assure that the conversion was done correctly, then display the value counts to assure that the column now only consists of 0 and 1 values and that the counts are identical to before."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"caad96eb"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"8b13a248","metadata":{},"source":["### Exercise 5.3\n","Split the data into two distinct sets of train and test data respectively.\n","Use the imported `train_test_split` function.\n","Visualise the output shapes to ensure that the ratio between the two is as expected.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c7207f50"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"f03025ca","metadata":{},"source":["## Tokenize the review texts\n","An important concept in natural language processing is tokenization.\n","As individual characters have little meaning and would yield an extremely high-dimensional search space, texts are instead tokenized into common sequences of characters.\n","For LLM's such as GPT, the tokenization is nowadays commonly done character-based.\n","However, especially for smaller models it is a good idea to use whole words as tokens."]},{"cell_type":"markdown","id":"5dc3b739","metadata":{},"source":["### Exercise 5.4\n","Use the keras Tokenizer (already imported for your convenience) to tokenize our input data.\n","#### Exercise 5.4.1\n","Instantiate the Tokenizer and fit it on the review texts of our **training data** using the `fit_on_texts()` method.\n","In this manner, the Tokenizer will determine how to tokenize data based on the provided fitting data.\n","You can directly pass in the Polars Series object, as it implements the required iterator for said method."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"5940cc46"},"outputs":[],"source":["tokenizer = Tokenizer(num_words=5000)"]},{"cell_type":"markdown","id":"3619bc9d","metadata":{},"source":["#### Exercise 5.4.2\n","Subsequently, convert the reviews from our training and test data to create the final input data (X) to use for training and testing using the `texts_to_sequences` method on the just created instance.\n","Display the resulting arrays to ensure that the text was tokenized and to get a rough idea of the input for our model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e98b4624"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"bb74f773","metadata":{},"source":["#### Exercise 5.4.3\n","Convert our training and test labels to numpy arrays.\n","Again inspect both arrays to ensure the content is roughly as expected."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"153a60a4"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"1bb33e93","metadata":{},"source":["## Defining and compiling the model\n","Here we build and train the model.\n","The model we will be using is a small model, and it looks simple because Keras defined a great deal for us.\n","Although the model itself is outside of the scope of this hackathon, of course I'll give a brief explanation of the different parts.\n","The Embedding layer defines takes the sparse input data and embeds it into a lower-dimensional space that is easier for our model to work with.\n","The output of this layer is passed into the LSTM (Long Short-Term Memory) submodel, which is a model consisting of multiple layers internally, designed to learn how to relate sequential data (e.g. words in a sentence).\n","Finally, the output of the LSTM is passed into a Dense layer with a Sigmoid activation function.\n","This layer will give a continuous output between 0 and 1.\n","During the training phase, we will use the loss function (\"binary cross-entropy\" as defined below) to steer the model towards predicting a score close to 0 for the labels we defined as 0, and 1 for the labels we defined as 1.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"651d9403","outputId":"e1e83cc0-24b5-49ef-c9d1-8ea85e0dc6ca"},"outputs":[],"source":["model = Sequential()\n","model.add(Embedding(input_dim=5000, output_dim=128, input_length=250))\n","model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(1, activation=\"sigmoid\"))\n","\n","model.summary()\n","\n","model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"]},{"cell_type":"markdown","metadata":{"id":"5ef6b455"},"source":["## Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9c8d3e3d","outputId":"c497b5ff-33cb-48d0-fc58-3fa631a1523c"},"outputs":[],"source":["model.fit(train_x, train_y, epochs=5, batch_size=64, validation_split=0.2)"]},{"cell_type":"markdown","id":"c54a4316","metadata":{},"source":["## Save the model and tokenizer\n","We save the model and tokenizer to be able to load them for a different purpose or in a later phase.\n","For the remainder of this notebook, however, we will keep using the instances we have here."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bbfa9987"},"outputs":[],"source":["model.save(\"model.h5\")\n","joblib.dump(tokenizer, \"tokenizer.pkl\")"]},{"cell_type":"markdown","id":"b2e8c53a","metadata":{},"source":["## Evaluate the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ESVTVqlnAiwr","outputId":"bb27202e-ac29-41b6-96c5-f9176392bd1a"},"outputs":[],"source":["model.evaluate(test_x, test_y)"]},{"cell_type":"markdown","id":"27493a8f","metadata":{},"source":["## Use the model to predict the sentiment for a review\n","### Exercise 5.5\n","Define a function that takes a review text as input and uses the tokenizer to convert the text to a token sequence, pad it similarly to the input data and finally provide it as input for our model.\n","Then read the output and convert it to a \"positive\" or \"negative\" string."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"STCsgeezE3pq"},"outputs":[],"source":["def predict_sentiment_for_review(review):\n","    # Implement me\n","    return None"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"TGRJ7uCQHA3h","outputId":"0d34b7a7-3130-44d4-e79a-ec7b1fb673fe"},"outputs":[],"source":["predict_sentiment_for_review(\"I'd rather watch paint dry\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"59qOFXkHHGtR"},"outputs":[],"source":["predict_sentiment_for_review(\"The best movie I've ever seen\")"]},{"cell_type":"code","execution_count":null,"id":"4cae7834","metadata":{},"outputs":[],"source":["predict_sentiment_for_review(\"Absolutely stunning!\")"]},{"cell_type":"code","execution_count":null,"id":"4dbd3348","metadata":{},"outputs":[],"source":["predict_sentiment_for_review(\"Even The Titanic was better...\")"]},{"cell_type":"code","execution_count":null,"id":"5171d3a3","metadata":{},"outputs":[],"source":["predict_sentiment_for_review(\"sucks\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":5}
